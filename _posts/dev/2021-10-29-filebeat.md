---
layout: post
title: Filebeat를 이용한 로그 데이터 수집
categories: [dev]
author: 김동현
email: kdh@jinhakapply.com
date: 2021-10-29
tag:
  - elk
  - elastic
  - logstash
  - filebeat
---

추후 캐치클래스에 도입할 사용자 패턴 로그 기록을 위한 조사 과정에서 엘라스틱 스택(ELK)을 잠정적으로 선택하게 되었습니다. 엘라스틱 서치는 주로 오픈소스 검색 엔진으로 많이 사용하지만, 그 자체로도 훌륭한 일종의 NoSQL 이기 때문에 데이터 저장 용도로도 많이 사용되고 있습니다. 또한 추가 기능 제공을 위한 오픈소스 프로젝트들이 Logstash, Kibana로 통합되고 정리되면서 로그 데이터의 가공, 시각화도 한 번에 정리할 수 있다는 장점이 있습니다.

Logstash, Elasticsearch 만으로도 로그 수집이 가능하지만, logstash는 비교적 무겁기도 하고 여러 마이크로서비스를 돌리거나 좀 더 유동적인 구성을 하고 싶은 경우 **경량화 수집 프로그램**인 Beats 를 이용하여 데이터를 전송하는 것이 좋습니다.

Beats 는 Metricbeat, Auditbeat, Filebeat 등 여러 종류가 있지만, 저희는 로그 파일의 변경을 추적하여 수집하는 것이 목적이기 때문에 Filebeat를 이용하게 되었습니다.

기본적인 로그 수집 프로세스는 다음과 같이 이루어집니다.

![sam2.png](/assets/img/posts/dev/2021-10-29-filebeat/sam2.png)

각각의 서버에 beat가 돌아가면서 데이터를 수집하고 이를 logstash 서버에 전송합니다. 곧바로 elasticsearch에 넣을 수도 있지만 logstash가 제공하는 여러 플러그인, 필터들을 사용하면 손쉽게 로그 데이터를 가공할 수 있습니다.

이제 elasticsearch에 저장된 데이터를 kibana로 시각화하여 파악하거나, 가공하여 사용할 수 있습니다.

실습은 docker 와 node.js(express.js)를 이용하여 간단한 예제를 통해 진행하였습니다.

```docker
FROM node:14 AS base
RUN mkdir /node-svr
WORKDIR /node-svr

COPY filebeat filebeat
RUN dpkg -i filebeat/filebeat-7.15.0-amd64.deb
COPY filebeat/filebeat.yml /etc/filebeat/filebeat.yml

COPY package.json .
COPY run.sh .

# ...

EXPOSE 3000
CMD ./run.sh
```

[elastic.co](http://elastic.co) 에서 리눅스용 filebeat 설치파일을 받아 dockerfile에서 복사 및 설치 후 미리 작업해 둔 환경설정 파일(filebeat.yml)을 붙여넣습니다. 별도로 설정하지 않는다면 기본적으로 filebeat는 /etc/filebeat 폴더에 설치되며, **filebeat.yml** 환경설정 파일이 생성됩니다.

공식 홈페이지에서는 설정할 수 있는 방식을 모두 기재해 놓은 [reference 파일](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-reference-yml.html)을 제공하고 있습니다. 여기서는 주의깊게 보아야 할 부분은 filebeat.inputs와 output.logstash 입니다.

```yaml
filebeat.inputs:
  # input 타입
  - type: log

    # 적용
    enabled: true

    # 추적할 로그 폴더. Glob 형식.
    paths:
      - /node-svr/logs/*.log
# ...

# output.elasticsearch:
#   # Elasticsearch 호스트 목록
#   hosts: ["172.17.0.1:9200"]

output.logstash:
  # Logstash 호스트 목록
  hosts: ["호스트 주소"]
```

inputs 타입에서 Docker, AWS, Kafka, Redis 등 여러 옵션을 제공하지만, 여기서는 서버에서 생성하는 로그 파일 추적을 위해 log(또는 filestream)를 사용하겠습니다.

```bash
#!/bin/bash

service filebeat start
npm run start
```

Dockerfile의 마지막 부분에서 CMD 명령은 하나만 실행할 수 있기 때문에 위와 같은 스크립트를 이용하여 filebeat와 node.js를 실행합니다.

이제 /node-svr/logs/ 하위에 로그 파일을 생성·변경하면 filebeat가 변경을 감지한 후에 logstash 호스트 주소로 데이터를 전송하게 됩니다. filebeat는 **각각의 파일의 상태를 저장**하고 있기 때문에, 모든 로그 변경이 전송되는 것을 보장합니다. 만일 logstash나 elasticsearch 서버의 오류 또는 통신 에러로 인해 전송이 실패한다 하여도 추후에 정상적인 상태에서 미처 **전송되지 못한 데이터를 전송**해준다는 장점이 있습니다.

그러나 데이터가 전송되기 전에 파일이 이동 또는 삭제되는 경우에는 데이터의 정합성을 보장해주진 않습니다.

다음으로 filebeat에서 전송한 데이터를 처리하는 logstash 부분을 간략히 살펴보겠습니다. Logstash는 데이터 파이프라인으로, 입력받은 데이터를 가공, 처리하여 출력해주는 중간 다리 역할을 합니다.

```
input {
	beats {
		port => 5044  # Beats 전송을 읽어들일 포트 번호
	}

	tcp {
		port => 5000  # TCP 연결을 통한 전송을 읽어들일 포트 번호
	}
}

filter {
	# JSON parsing plugin
  json {
    source => "message"
  }
}

output {
	# beat 메타데이터가 filebeat일 경우 처리
  if [@metadata][beat] == "filebeat" {
    elasticsearch {
      hosts => "elasticsearch:9200"  # 엘라스틱 서치 호스트
      user => "elastic_id"  # 엘라스틱 서치 아이디
      password => "elastic_password" # 엘라스틱 서치 비밀번호
      ecs_compatibility => disabled  # Elastic Common Schema 보장 여부
      index => "filebeat-%{+YYYY}"  # 인덱스명
    }
  }
}
```

5044 포트로 들어온 Filebeat에서 전송된 데이터들은 Json 플러그인을 거치면서 message 필드에 입력되고, 9200 포트의 elasticsearch에 filebeat-{현재 연도} 형식 인덱스로 저장됩니다.

![sam.jpg](/assets/img/posts/dev/2021-10-29-filebeat/sam.jpg)

Kibana dev tool

정상적으로 작동시 위와 같이 인덱스에 맞게 호스트 및 agent 정보, timestamp와 로그 데이터가 elasticsearch에 저장됩니다.

이로써 기본적인 filebeat 로그 데이터 전송 및 저장에 대해 알아보았습니다. 추후에는 Kibana를 이용하여 로그 데이터를 시각화하고 분석하는 방법에 대해 알아보겠습니다.
